{
  "linearModels":[
    {
      "nameAlgorithm": "Bayesian Regression",
      "definition": "Bayesian Regression techniques serve to include regularization in the estimation procedure. We select the regularization parameter, not in an entirely mathematically rigorous way, but empirically to work best with the dataset. To do so, we introduce uninformative priors over the parameters of the model. The regularization used in Ridge Regression is equivalent to finding an optimum a posteriori estimation, which we assume to follow a Gaussian distribution, prior over the coefficients with precision. Instead of setting lambda manually, it is possible to treat it as a random variable and estimate it from the data.",
      "params" : "."
    },

    {
      "nameAlgorithm": "Elastic-Net",
      "definition": "ElasticNet is a linear regression model trained with both no and -norm regularization of the coefficients. This combination allows for learning a sparse model where few of the weights are non-zero like Lasso, while still maintaining the regularization properties of Ridge. We control the convex combination of and using the l1_ratio parameter.",
      "params" : "."
    },

    {
      "nameAlgorithm": "LARS Lasso",
      "definition": "LassoLars is a lasso model implemented using the LARS algorithm. Unlike the implementation based on coordinate descent, this yields the exact solution, which is piecewise linear as a function of the norm of its coefficients.",
      "params" : "Without parameters."
    },

    {
      "nameAlgorithm": "Lasso",
      "definition": "Lasso is a linear model that estimates sparse coefficients. It is useful in some contexts due to its tendency to prefer solutions with fewer non-zero coefficients, effectively reducing the number of features upon which the given solution is dependent. For this reason, Lasso and its variants are fundamental to the field of compressed sensing. Under certain conditions, it can recover the exact set of non-zero coefficients.",
      "params" : "."
    },

    {
      "nameAlgorithm": "Ordinal Least Square",
      "definition": "LinearRegression fits a linear model with coefficients to minimize the residual sum of squares between the observed targets in the dataset, and the targets predicted by the linear approximation",
      "params" : "."
    },

    {
      "nameAlgorithm": "Ridge Regession",
      "definition": "Ridge regression addresses some of the problems of Ordinary Least Squares by imposing a penalty on the size of the coefficients. The ridge coefficients minimize a penalized residual sum of squares",
      "params" : "."
    }
  ],

  "classificationAlgorithm":[
    {
      "nameAlgorithm": "AdaBoostClassifier",
      "definition": "An AdaBoost classifier is a meta-estimator that begins by fitting a classifier on the original dataset and then fits additional copies of the classifier on the same dataset but where the weights of incorrectly classified instances are adjusted such that subsequent classifiers focus more on severe cases.",
      "params" : "n_estimators: The maximum number of estimators at which boosting is terminated. In case of perfect fit, the learning procedure is stopped early.If ‘SAMME.R’ then use the SAMME.R real boosting algorithm. base_estimator must support calculation of class probabilities. algorithm: If ‘SAMME’ then use the SAMME discrete boosting algorithm. The SAMME.R algorithm typically converges faster than SAMME, achieving a lower test error with fewer boosting iterations."
    },

    {
      "nameAlgorithm": "BaggingClassifier",
      "definition": "A Bagging classifier is an ensemble meta-estimator that fits base classifiers each on random subsets of the original dataset and then aggregate their individual predictions (either by voting or by averaging) to form a final prediction. Such a meta-estimator can typically be used as a way to reduce the variance of a black-box estimator (e.g., a decision tree), by introducing randomization into its construction procedure and then making an ensemble out of it.",
      "params" : "n_estimators: The number of base estimators in the ensemble."
    },

    {
      "nameAlgorithm": "BernoulliNB",
      "definition": "Naive Bayes classifier for multivariate Bernoulli models. Naive Bayes methods are a set of supervised learning algorithms based on applying Bayes’ theorem with the “naive” assumption of conditional independence between every pair of features given the value of the class variable. Like MultinomialNB, this classifier is suitable for discrete data. The difference is that while MultinomialNB works with occurrence counts, BernoulliNB is designed for binary/boolean features.",
      "params" : "Without parameters."
    },

    {
      "nameAlgorithm": "DecisionTree",
      "definition": "The goal is to create a model that predicts the value of a target variable by learning simple decision rules inferred from the data features.",
      "params" : "criterion: The goal is to create a model that predicts the value of a target variable by learning simple decision rules inferred from the data features. splitter:The strategy used to choose the split at each node. Supported strategies are “best” to choose the best split and “random” to choose the best random split."
    },

    {
      "nameAlgorithm": "GaussianNB",
      "definition": "Implements the Gaussian Naive Bayes algorithm for classification. The likelihood of the features is assumed to be Gaussian.",
      "params" : "Without parameters."
    },

    {
      "nameAlgorithm": "GradientBoostingClassifier",
      "definition": "GB builds an additive model in a forward stage-wise fashion; it allows for the optimization of arbitrary differentiable loss functions. In each stage n_classes_ regression trees are fit on the negative gradient of the binomial or multinomial deviance loss function. Binary classification is a particular case where only a single regression tree is induced.",
      "params" : "n_estimators: The number of boosting stages to perform. Gradient boosting is fairly robust to over-fitting so a large number usually results in better performance. criterion: The function to measure the quality of a split. Supported criteria are “friedman_mse” for the mean squared error with improvement score by Friedman, “mse” for mean squared error, and 'mae' for the mean absolute error. The default value of 'friedman_mse' is generally the best as it can provide a better approximation in some cases."
    },

    {
      "nameAlgorithm": "KNeighborsClassifier",
      "definition": "KNeighborsClassifier implements learning based on the nearest neighbors of each query point, where is an integer value specified by the user. ",
      "params" : "n_neighbors: Number of neighbors to use by default for kneighbors queries. algorithm :Algorithm used to compute the nearest neighbors: ‘ball_tree’ will use BallTree. ‘kd_tree’ will use KDTree. ‘brute’ will use a brute-force search. ‘auto’ will attempt to decide the most appropriate algorithm based on the values passed to fit method."
    },

    {
      "nameAlgorithm": "MLPClassifier",
      "definition": "This model optimizes the log-loss function using LBFGS or stochastic gradient descent.",
      "params" : "activation: Activation function for the hidden layer. ‘identity’, no-op activation, useful to implement linear bottleneck, returns f(x) = x. ‘logistic’, the logistic sigmoid function, returns f(x) = 1 / (1 + exp(-x)). ‘tanh’, the hyperbolic tan function, returns f(x) = tanh(x). ‘relu’, the rectified linear unit function, returns f(x) = max(0, x). solver : The solver for weight optimization. ‘lbfgs’ is an optimizer in the family of quasi-Newton methods. ‘sgd’ refers to stochastic gradient descent. ‘adam’ refers to a stochastic gradient-based optimizer proposed by Kingma, Diederik, and Jimmy Ba. learning_rate : Learning rate schedule for weight updates. ‘constant’ is a constant learning rate given by ‘learning_rate_init’. ‘invscaling’ gradually decreases the learning rate at each time step ‘t’ using an inverse scaling exponent of ‘power_t’. effective_learning_rate = learning_rate_init / pow(t, power_t). ‘adaptive’ keeps the learning rate constant to ‘learning_rate_init’ as long as training loss keeps decreasing. Each time two consecutive epochs fail to decrease training loss by at least tol, or fail to increase validation score by at least tol if ‘early_stopping’ is on, the current learning rate is divided by 5. n_layers_ : Number of layers."
    },

    {
      "nameAlgorithm": "NuSVC",
      "definition": "Nu-Support Vector Classification. Similar to SVC but uses a parameter to control the number of support vectors. The implementation is based on libsvm.",
      "params" : "kernel: Specifies the kernel type to be used in the algorithm. It must be one of ‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’, ‘precomputed’ or a callable. If none is given, ‘rbf’ will be used. If a callable is given it is used to precompute the kernel matrix. gamma: Kernel coefficient for ‘rbf’, ‘poly’ and ‘sigmoid’. Current default is ‘auto’ which uses 1 / n_features, if gamma='scale' is passed then it uses 1 / (n_features * X.std()) as value of gamma. The current default of gamma, ‘auto’, will change to ‘scale’ in version 0.22. ‘auto_deprecated’, a deprecated version of ‘auto’ is used as a default indicating that no explicit value of gamma was passed."
    },

    {
      "nameAlgorithm": "RandomForestClassifier",
      "definition": "A random forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
      "params" : "n_estimators: The number of trees in the forest. Changed in version 0.20: The default value of n_estimators will change from 10 in version 0.20 to 100 in version 0.22. criterion : The function to measure the quality of a split. Supported criteria are “gini” for the Gini impurity and “entropy” for the information gain. Note: this parameter is tree-specific."
    },

    {
      "nameAlgorithm": "SVC",
      "definition": "C-Support Vector Classification. The implementation is based on libsvm. The fit time complexity is more than quadratic with the number of samples which makes it hard to scale to dataset with more than a couple of 10000 samples. The multiclass support is handled according to a one-vs-one scheme",
      "params" : "kernel : Specifies the kernel type to be used in the algorithm. It must be one of ‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’, ‘precomputed’ or a callable. If none is given, ‘rbf’ will be used. If a callable is given it is used to pre-compute the kernel matrix from data matrices; that matrix should be an array of shape (n_samples, n_samples). gamma : Kernel coefficient for ‘rbf’, ‘poly’ and ‘sigmoid’. Current default is ‘auto’ which uses 1 / n_features, if gamma='scale' is passed then it uses 1 / (n_features * X.std()) as value of gamma. The current default of gamma, ‘auto’, will change to ‘scale’ in version 0.22. ‘auto_deprecated’, a deprecated version of ‘auto’ is used as a default indicating that no explicit value of gamma was passed"
    }
  ],

  "definitionPerformanceCla":[

    {
      "namePerformance": "Precision",
      "definition" : "In the field of information retrieval, precision is the fraction of retrieved documents that are relevant to the query. Precision is used with recall, the percent of all relevant documents that is returned by the search. The two measures are sometimes used together in the F1 Score (or f-measure) to provide a single measurement for a system."
    },

    {
      "namePerformance": "Accuracy",
      "definition" : "Accuracy is a statistical measure of how well a classification test correctly identifies or excludes a condition. That is, the accuracy is the proportion of true results (both true positives and true negatives) among the total number of cases examined. A value close to 100% indicates a high accuracy of the model, while values below 60% indicate that the classification delivers random elements and does not generate adequate confidence, so the model requires refinement or simply should be discarded."
    },

    {
      "namePerformance": "Recall",
      "definition" : "In information retrieval, recall is the fraction of the relevant documents that are successfully retrieved. Precision is used with recall, the percent of all relevant documents that is returned by the search. The two measures are sometimes used together in the F1 Score (or f-measure) to provide a single measurement for a system."
    },

    {
      "namePerformance": "NegLogLoss",
      "definition" : "."
    },

    {
      "namePerformance": "F1 Score",
      "definition" : "The F1 score can be interpreted as a weighted average of the precision and recall, where an F1 score reaches its best value at 1 and worst score at 0. The relative contribution of precision and recall to the F1 score are equal. "
    },

    {
      "namePerformance": "FBeta Score",
      "definition" : "."
    }
  ],

  "curveRepresentation":[

    {
      "nameCurve": "Confusion Matrix",
      "definition" : "A Confusion Matrix is a specific table layout that allows visualization of the performance of an algorithm, typically a supervised learning one (in unsupervised learning it is usually called a matching matrix). Each row of the matrix represents the instances in a predicted class while each column represents the instances in an actual class (or vice versa). The name stems from the fact that it makes it easy to see if the system is confusing two classes (i.e. commonly mislabeling one as another)."
    },

    {
      "nameCurve": "ROC Curve",
      "definition" : "."
    },

    {
      "nameCurve": "Learning Curve",
      "definition" : "A learning curve shows the validation and training score of an estimator for varying numbers of training samples. It is a tool to find out how much we benefit from adding more training data and whether the estimator suffers more from a variance error or a bias error. If both the validation score and the training score converge to a value that is too low with increasing size of the training set, we will not benefit much from more training data. In the following plot, the user can see an example: naive Bayes roughly converges to a low score."
    },

    {
      "nameCurve": "Precision-Recall Curve",
      "definition" : "."
    }
  ],

  "predictionAlgorithm":[

    {
      "nameAlgorithm": "AdaBoostRegressor",
      "definition": "GB builds an additive model in a forward stage-wise fashion; it allows for the optimization of arbitrary differentiable loss functions. In each stage a regression tree is fit on the negative gradient of the given loss function.",
      "params" : "."
    },

    {
      "nameAlgorithm": "BaggingRegressor",
      "definition": "A Bagging regressor is an ensemble meta-estimator that fits base regressors each on random subsets of the original dataset and then aggregate their individual predictions (either by voting or by averaging) to form a final prediction. Such a meta-estimator can typically be used as a way to reduce the variance of a black-box estimator (e.g., a decision tree), by introducing randomization into its construction procedure and then making an ensemble out of it. This algorithm encompasses several works from the literature. When random subsets of the dataset are drawn as random subsets of the samples, then this algorithm is known as Pasting. If samples are drawn with replacement, then the method is known as Bagging. When random subsets of the dataset are drawn as random subsets of the features, then the method is known as Random Subspaces. Finally, when base estimators are built on subsets of both samples and features, then the method is known as Random Patches.",
      "params" : "."
    },

    {
      "nameAlgorithm": "DecisionTree",
      "definition": "As in the classification setting, the fit method will take as argument arrays X and y, only that in this case y is expected to have floating point values instead of integer values.",
      "params" : "."
    },

    {
      "nameAlgorithm": "GradientBoostingRegressor",
      "definition": "GB builds an additive model in a forward stage-wise fashion; it allows for the optimization of arbitrary differentiable loss functions. In each stage a regression tree is fit on the negative gradient of the given loss function.",
      "params" : "."
    },

    {
      "nameAlgorithm": "KNeighborsRegressor",
      "definition": "Regression based on k-nearest neighbors. The target is predicted by local interpolation of the targets associated of the nearest neighbors in the training set.",
      "params" : "."
    },

    {
      "nameAlgorithm": "MLPRegressor",
      "definition": "This model optimizes the squared-loss using LBFGS or stochastic gradient descent.",
      "params" : "."
    },

    {
      "nameAlgorithm": "NuSVR",
      "definition": "Similar to NuSVC, for regression, uses a parameter nu to control the number of support vectors. However, unlike NuSVC, where nu replaces C, here nu replaces the parameter epsilon of epsilon-SVR.",
      "params" : "."
    },

    {
      "nameAlgorithm": "RandomForestRegressor",
      "definition": "A random forest is a meta estimator that fits a number of classifying decision trees on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).",
      "params" : "."
    },

    {
      "nameAlgorithm": "SVR",
      "definition": "Epsilon-Support Vector Regression. The free parameters in the model are C and epsilon. The implementation is based on libsvm.",
      "params" : "."
    }

  ],

  "resultPrediction":[

    {
      "namePerformance": "R Score",
      "definition" : "."
    }
  ],

  "characteristic":[

    {
      "namePerformance": "PCA",
      "definition" : "."
    },

    {
      "namePerformance": "Kernel PCA",
      "definition" : "."
    },

    {
      "namePerformance": "Correlation Analysis",
      "definition" : "."
    },

    {
      "namePerformance": "Mutual Information",
      "definition" : "."
    },

    {
      "namePerformance": "Incremental PCA",
      "definition" : "."
    }
  ],

  "clusteringAlgorithm":[
    {
      "nameAlgorithm": "K-Means",
      "definition": "The KMeans algorithm clusters data by trying to separate samples in n groups of equal variance, minimizing a criterion known as the inertia or within-cluster sum-of-squares. This algorithm requires the number of clusters to be specified. It scales well to large number of samples and has been used across a large range of application areas in many different fields.",
      "params" : "."
    },

    {
      "nameAlgorithm": "Birch",
      "definition": "The Birch builds a tree called the Characteristic Feature Tree (CFT) for the given data. The data is essentially lossy compressed to a set of Characteristic Feature nodes (CF Nodes). The CF Nodes have a number of subclusters called Characteristic Feature subclusters (CF Subclusters) and these CF Subclusters located in the non-terminal CF Nodes can have CF Nodes as children.",
      "params" : "."
    },

    {
      "nameAlgorithm": "Agglomerative",
      "definition": "The AgglomerativeClustering object performs a hierarchical clustering using a bottom up approach: each observation starts in its own cluster, and clusters are successively merged together. The linkage criteria determines the metric used for the merge strategy.",
      "params" : "."
    },

    {
      "nameAlgorithm": "Affinity Propagation",
      "definition": "AffinityPropagation creates clusters by sending messages between pairs of samples until convergence. A dataset is then described using a small number of exemplars, which are identified as those most representative of other samples. The messages sent between pairs represent the suitability for one sample to be the exemplar of the other, which is updated in response to the values from other pairs. This updating happens iteratively until convergence, at which point the final exemplars are chosen, and hence the final clustering is given.",
      "params" : "."
    },

    {
      "nameAlgorithm": "Mean Shift",
      "definition": "Mean shift clustering aims to discover “blobs” in a smooth density of samples. It is a centroid-based algorithm, which works by updating candidates for centroids to be the mean of the points within a given region. These candidates are then filtered in a post-processing stage to eliminate near-duplicates to form the final set of centroids.",
      "params" : "."
    },

    {
      "nameAlgorithm": "DBScan",
      "definition": "DBSCAN - Density-Based Spatial Clustering of Applications with Noise. Finds core samples of high density and expands clusters from them. Good for data which contains clusters of similar density.",
      "params" : "."
    }
  ],

  "definitionPerformanceClustering":[

    {
      "namePerformance": "Calinski-Harabaz Index",
      "definition" : "."
    },

    {
      "namePerformance": "Silhouette Coefficient",
      "definition" : "."
    }
  ]
}
